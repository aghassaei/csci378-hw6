{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2000, 2000, 2000, 2000, 2000]\n",
      "A=0: [1376, 1313, 1168, 783, 360]\n",
      "A=1: [624, 687, 832, 1217, 1640]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure the _demographic parity_ of our classifier: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$. In the the function below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity(R, A):\n",
    "   return torch.mean(R[A == 0], axis=0) - torch.mean(R[A == 1], axis=0)\n",
    "\n",
    "def equalized_odds(R, A, Y):\n",
    "   return torch.mean(R[(A == 0) & (Y == 1)], axis=0) - torch.mean(R[(A == 1) & (Y == 1)], axis=0)\n",
    "\n",
    "# def predictive_parity(R, A, Y):\n",
    "#    print(\"R[A = 0]:\", R[A==0].size(), \"R[A == 1]:\", R[A==1].size())\n",
    "#    print(\"R[A = 0][Y == 1]:\", R[A==0][Y == 1].size(), \"R[A == 1][Y == 1]:\", R[A==1][Y == 1].size())\n",
    "#    return torch.mean(R[A == 0], axis=0) - torch.mean(Y[(A == 1) & (R == 1)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200, fairness=demographic_parity, attributes=A):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    attributes = torch.tensor(attributes)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            if fairness == equalized_odds:\n",
    "                bias = fairness(probs, attributes, data_out)\n",
    "            else:\n",
    "                bias = fairness(probs, attributes)\n",
    "\n",
    "\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", ['%.4f' % b for b in bias])\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias measure: demographic parity\n",
      "Epoch: 19 Accuracy: 0.3240000009536743 Bias: ['0.1775', '0.1574', '0.1035', '-0.0441', '-0.3943']\n",
      "Epoch: 39 Accuracy: 0.34940001368522644 Bias: ['0.1919', '0.1696', '0.0997', '-0.0665', '-0.3947']\n",
      "Epoch: 59 Accuracy: 0.3919999897480011 Bias: ['0.1996', '0.1743', '0.0950', '-0.0703', '-0.3986']\n",
      "Epoch: 79 Accuracy: 0.45730000734329224 Bias: ['0.1969', '0.1726', '0.0933', '-0.0552', '-0.4075']\n",
      "Epoch: 99 Accuracy: 0.5371999740600586 Bias: ['0.1890', '0.1672', '0.0904', '-0.0415', '-0.4051']\n",
      "Epoch: 119 Accuracy: 0.6010000109672546 Bias: ['0.1849', '0.1643', '0.0893', '-0.0446', '-0.3939']\n",
      "Epoch: 139 Accuracy: 0.637499988079071 Bias: ['0.1830', '0.1612', '0.0907', '-0.0474', '-0.3875']\n",
      "Epoch: 159 Accuracy: 0.6672000288963318 Bias: ['0.1818', '0.1583', '0.0932', '-0.0504', '-0.3829']\n",
      "Epoch: 179 Accuracy: 0.7046999931335449 Bias: ['0.1745', '0.1505', '0.0962', '-0.0380', '-0.3831']\n",
      "Epoch: 199 Accuracy: 0.761900007724762 Bias: ['0.1620', '0.1362', '0.0741', '-0.0146', '-0.3577']\n",
      "Epoch: 219 Accuracy: 0.704800009727478 Bias: ['0.1955', '0.1545', '0.0927', '-0.0725', '-0.3702']\n",
      "Epoch: 239 Accuracy: 0.72079998254776 Bias: ['0.1833', '0.1534', '0.1105', '-0.0761', '-0.3710']\n",
      "Epoch: 259 Accuracy: 0.7731000185012817 Bias: ['0.1600', '0.1295', '0.1026', '-0.0167', '-0.3754']\n",
      "Epoch: 279 Accuracy: 0.79339998960495 Bias: ['0.1683', '0.1270', '0.0808', '-0.0277', '-0.3484']\n",
      "Epoch: 299 Accuracy: 0.7724000215530396 Bias: ['0.1862', '0.1351', '0.0906', '-0.0547', '-0.3571']\n",
      "\n",
      "Bias measure: equalized odds\n",
      "Epoch: 19 Accuracy: 0.3334999978542328 Bias: ['0.1170', '0.0960', '0.0370', '-0.0800', '-0.1690']\n",
      "Epoch: 39 Accuracy: 0.359499990940094 Bias: ['0.1280', '0.0970', '0.0060', '-0.1210', '-0.1100']\n",
      "Epoch: 59 Accuracy: 0.3984000086784363 Bias: ['0.1450', '0.1020', '-0.0200', '-0.1380', '-0.0890']\n",
      "Epoch: 79 Accuracy: 0.45899999141693115 Bias: ['0.1570', '0.1020', '-0.0440', '-0.1350', '-0.0800']\n",
      "Epoch: 99 Accuracy: 0.5332000255584717 Bias: ['0.1610', '0.1030', '-0.0670', '-0.1260', '-0.0710']\n",
      "Epoch: 119 Accuracy: 0.5960999727249146 Bias: ['0.1550', '0.1080', '-0.0850', '-0.1260', '-0.0510']\n",
      "Epoch: 139 Accuracy: 0.6385999917984009 Bias: ['0.1450', '0.1160', '-0.1000', '-0.1250', '-0.0360']\n",
      "Epoch: 159 Accuracy: 0.6711000204086304 Bias: ['0.1340', '0.1280', '-0.1130', '-0.1220', '-0.0280']\n",
      "Epoch: 179 Accuracy: 0.6920999884605408 Bias: ['0.1270', '0.1410', '-0.1270', '-0.1190', '-0.0230']\n",
      "Epoch: 199 Accuracy: 0.6848000288009644 Bias: ['0.1430', '0.2000', '-0.1600', '-0.1610', '-0.0220']\n",
      "Epoch: 219 Accuracy: 0.7053999900817871 Bias: ['-0.3080', '0.1650', '0.1310', '0.0120', '0.0000']\n",
      "Epoch: 239 Accuracy: 0.699400007724762 Bias: ['-0.2750', '0.1890', '0.0760', '0.0110', '-0.0000']\n",
      "Epoch: 259 Accuracy: 0.7046999931335449 Bias: ['-0.3040', '0.2120', '0.0810', '0.0110', '-0.0000']\n",
      "Epoch: 279 Accuracy: 0.7121999859809875 Bias: ['-0.3170', '0.2270', '0.0820', '0.0090', '-0.0000']\n",
      "Epoch: 299 Accuracy: 0.7688999772071838 Bias: ['-0.2930', '0.2170', '0.0720', '0.0030', '-0.0000']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bias measure: demographic parity\")\n",
    "model_unfair_dp = train_unfair(lr=5e-1, epochs=300)\n",
    "\n",
    "print(\"\\nBias measure: equalized odds\")\n",
    "model_unfair_eq = train_unfair(lr=5e-1, epochs=300, fairness=equalized_odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2568, 1953, 1425, 3076, 978]\n",
      "A=0: [1472, 1354, 726, 1263, 185]\n",
      "A=1: [1096, 599, 699, 1813, 793]\n"
     ]
    }
   ],
   "source": [
    "p = model_unfair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Compute the Lagrangian loss L + lam * g\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        probs = nn.functional.softmax(preds, dim=1)\n",
    "        bias = demographic_parity(probs, A)\n",
    "        loss_val += (lam * bias).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "\n",
    "            \n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", ['%.4f' % b for b in demographic_parity(probs, A)], \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.22579999268054962 Bias: ['-0.0419', '-0.0279', '-0.0149', '0.0269', '0.0578'] Lambda: 0.23635146021842957\n",
      "Epoch: 39 Accuracy: 0.2705000042915344 Bias: ['-0.0286', '-0.0238', '-0.0123', '0.0060', '0.0587'] Lambda: 0.2752276360988617\n",
      "Epoch: 59 Accuracy: 0.2896000146865845 Bias: ['-0.0368', '-0.0296', '-0.0146', '0.0101', '0.0709'] Lambda: 0.3086240589618683\n",
      "Epoch: 79 Accuracy: 0.3675999939441681 Bias: ['-0.0449', '-0.0326', '-0.0114', '0.0091', '0.0797'] Lambda: 0.3257610499858856\n",
      "Epoch: 99 Accuracy: 0.41429999470710754 Bias: ['-0.0432', '-0.0282', '-0.0044', '0.0022', '0.0736'] Lambda: 0.3727899193763733\n",
      "Epoch: 119 Accuracy: 0.4334999918937683 Bias: ['-0.0429', '-0.0263', '-0.0014', '-0.0021', '0.0728'] Lambda: 0.45134952664375305\n",
      "Epoch: 139 Accuracy: 0.45019999146461487 Bias: ['-0.0428', '-0.0243', '0.0008', '-0.0046', '0.0709'] Lambda: 0.5334129333496094\n",
      "Epoch: 159 Accuracy: 0.45989999175071716 Bias: ['-0.0429', '-0.0220', '0.0027', '-0.0065', '0.0687'] Lambda: 0.6082276701927185\n",
      "Epoch: 179 Accuracy: 0.4668000042438507 Bias: ['-0.0434', '-0.0198', '0.0042', '-0.0070', '0.0660'] Lambda: 0.6752977967262268\n",
      "Epoch: 199 Accuracy: 0.4733000099658966 Bias: ['-0.0444', '-0.0181', '0.0057', '-0.0070', '0.0638'] Lambda: 0.7341338992118835\n",
      "Epoch: 219 Accuracy: 0.4832000136375427 Bias: ['-0.0388', '-0.0156', '0.0052', '-0.0133', '0.0626'] Lambda: 0.7861849069595337\n",
      "Epoch: 239 Accuracy: 0.5076000094413757 Bias: ['-0.0251', '-0.0111', '0.0014', '-0.0264', '0.0611'] Lambda: 0.8259175419807434\n",
      "Epoch: 259 Accuracy: 0.7014999985694885 Bias: ['0.0684', '0.0144', '-0.0105', '-0.0416', '-0.0306'] Lambda: 0.8662698268890381\n",
      "Epoch: 279 Accuracy: 0.6326000094413757 Bias: ['0.0513', '0.0117', '-0.0128', '0.0153', '-0.0655'] Lambda: 0.9131195545196533\n",
      "Epoch: 299 Accuracy: 0.638700008392334 Bias: ['0.0433', '0.0111', '-0.0101', '0.0108', '-0.0551'] Lambda: 0.9325528740882874\n"
     ]
    }
   ],
   "source": [
    "model_fair = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9402424e-6193-4550-a458-5eae0b3557a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [1731, 1843, 1449, 1939, 3038]\n",
      "A=0: [1140, 971, 625, 803, 1461]\n",
      "A=1: [591, 872, 824, 1136, 1577]\n"
     ]
    }
   ],
   "source": [
    "p = model_fair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        preds = network(data_in)\n",
    "        probs = nn.functional.softmax(preds, dim = 1)\n",
    "\n",
    "        # P = prob((R = r) | (A = 0))\n",
    "        P = torch.mean(probs[A==0], axis = 0)\n",
    "\n",
    "        # Q = prob((R = r) | (A = 1))\n",
    "        Q = torch.mean(probs[A==1], axis = 0)\n",
    "\n",
    "        # Summation\n",
    "        kl_log = torch.log(P / Q)\n",
    "        kl_div = torch.sum(P * kl_log)\n",
    "\n",
    "        loss_val = loss(preds, data_out) + lam * (kl_div - epsilon) \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.3160000145435333 Divergence: 0.0017289668321609497 Lambda: 0.8826927542686462\n",
      "Epoch: 39 Accuracy: 0.3174000084400177 Divergence: 0.0015972619876265526 Lambda: 1.212470531463623\n",
      "Epoch: 59 Accuracy: 0.32179999351501465 Divergence: 0.0022437660954892635 Lambda: 1.5176353454589844\n",
      "Epoch: 79 Accuracy: 0.328000009059906 Divergence: 0.003057121764868498 Lambda: 1.7916920185089111\n",
      "Epoch: 99 Accuracy: 0.34700000286102295 Divergence: 0.004950245842337608 Lambda: 2.0657811164855957\n",
      "Epoch: 119 Accuracy: 0.3714999854564667 Divergence: 0.01033196784555912 Lambda: 2.398014545440674\n",
      "Epoch: 139 Accuracy: 0.38269999623298645 Divergence: 0.016513975337147713 Lambda: 2.826582193374634\n",
      "Epoch: 159 Accuracy: 0.41290000081062317 Divergence: 0.024347763508558273 Lambda: 3.4310302734375\n",
      "Epoch: 179 Accuracy: 0.428600013256073 Divergence: 0.019872542470693588 Lambda: 4.006704807281494\n",
      "Epoch: 199 Accuracy: 0.45649999380111694 Divergence: 0.013929116539657116 Lambda: 4.508157730102539\n",
      "Epoch: 219 Accuracy: 0.4269999861717224 Divergence: 0.01580106094479561 Lambda: 5.057204246520996\n",
      "Epoch: 239 Accuracy: 0.45750001072883606 Divergence: 0.016974903643131256 Lambda: 5.390661716461182\n",
      "Epoch: 259 Accuracy: 0.42750000953674316 Divergence: 0.021382639184594154 Lambda: 5.797745704650879\n",
      "Epoch: 279 Accuracy: 0.4799000024795532 Divergence: 0.015815773978829384 Lambda: 6.102495193481445\n",
      "Epoch: 299 Accuracy: 0.4691999852657318 Divergence: 0.01600077748298645 Lambda: 6.623249530792236\n"
     ]
    }
   ],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4af25af0-dfe6-4474-8ebe-544359ff98ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2311, 1872, 1897, 1118, 2802]\n",
      "A=0: [1399, 961, 1089, 594, 957]\n",
      "A=1: [912, 911, 808, 524, 1845]\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
