{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure the _demographic parity_ of our classifier: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$. In the the function below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity(R, A):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_unfair(lr=5e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Compute the loss value as defined in the Lagrangian above\n",
    "        loss_val = ???\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A), \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402424e-6193-4550-a458-5eae0b3557a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Implement the loss function above here.\n",
    "        loss_val = ???\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25af0-dfe6-4474-8ebe-544359ff98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1ed3-96d7-4904-ae67-289010de748d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
