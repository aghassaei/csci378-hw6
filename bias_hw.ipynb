{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d6e9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: 10,000 observations with 0 or 1 for the sensitive attribute\n",
    "# X: 10,000 observations with 40 features\n",
    "# Y: 10,000 observations with outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2000, 2000, 2000, 2000, 2000]\n",
      "A=0: [1412, 1295, 1119, 805, 369]\n",
      "A=1: [588, 705, 881, 1195, 1631]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure the _demographic parity_ of our classifier: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$. In the the function below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R is output of nn after softmax\n",
    "# A is sensitive attributes\n",
    "\n",
    "def demographic_parity(R, A):\n",
    "   # R0 = R where corresponding index in A is 0\n",
    "   # R1 same but for 1\n",
    "   R0 = R[A == 0]\n",
    "   R1 = R[A == 1]\n",
    "\n",
    "   R0_av = torch.mean(R0, axis=0)\n",
    "   R1_av = torch.mean(R1, axis=0)\n",
    "\n",
    "   output = R1_av - R0_av\n",
    "\n",
    "   return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A))\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.3312000036239624 Bias: tensor([-0.1802, -0.1489, -0.0848,  0.0227,  0.3911], grad_fn=<SubBackward0>)\n",
      "Epoch: 39 Accuracy: 0.35249999165534973 Bias: tensor([-0.1962, -0.1617, -0.0908,  0.0460,  0.4027], grad_fn=<SubBackward0>)\n",
      "Epoch: 59 Accuracy: 0.4052000045776367 Bias: tensor([-0.2005, -0.1661, -0.0885,  0.0488,  0.4063], grad_fn=<SubBackward0>)\n",
      "Epoch: 79 Accuracy: 0.5095999836921692 Bias: tensor([-0.1952, -0.1621, -0.0844,  0.0367,  0.4050], grad_fn=<SubBackward0>)\n",
      "Epoch: 99 Accuracy: 0.5853000283241272 Bias: tensor([-0.1943, -0.1599, -0.0809,  0.0367,  0.3984], grad_fn=<SubBackward0>)\n",
      "Epoch: 119 Accuracy: 0.6283000111579895 Bias: tensor([-0.1953, -0.1574, -0.0780,  0.0405,  0.3903], grad_fn=<SubBackward0>)\n",
      "Epoch: 139 Accuracy: 0.6560999751091003 Bias: tensor([-0.1960, -0.1546, -0.0768,  0.0435,  0.3839], grad_fn=<SubBackward0>)\n",
      "Epoch: 159 Accuracy: 0.6787999868392944 Bias: tensor([-0.1967, -0.1517, -0.0767,  0.0464,  0.3788], grad_fn=<SubBackward0>)\n",
      "Epoch: 179 Accuracy: 0.6970000267028809 Bias: tensor([-0.1976, -0.1483, -0.0767,  0.0489,  0.3737], grad_fn=<SubBackward0>)\n",
      "Epoch: 199 Accuracy: 0.711899995803833 Bias: tensor([-0.1990, -0.1454, -0.0766,  0.0535,  0.3675], grad_fn=<SubBackward0>)\n",
      "Epoch: 219 Accuracy: 0.7142000198364258 Bias: tensor([-0.2050, -0.1467, -0.0776,  0.0686,  0.3608], grad_fn=<SubBackward0>)\n",
      "Epoch: 239 Accuracy: 0.6898999810218811 Bias: tensor([-0.2226, -0.1604, -0.0848,  0.1098,  0.3580], grad_fn=<SubBackward0>)\n",
      "Epoch: 259 Accuracy: 0.6858000159263611 Bias: tensor([-0.2204, -0.1533, -0.0843,  0.0805,  0.3775], grad_fn=<SubBackward0>)\n",
      "Epoch: 279 Accuracy: 0.7233999967575073 Bias: tensor([-0.2146, -0.1470, -0.0840,  0.0893,  0.3563], grad_fn=<SubBackward0>)\n",
      "Epoch: 299 Accuracy: 0.738099992275238 Bias: tensor([-0.2117, -0.1445, -0.0858,  0.0874,  0.3547], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_unfair = train_unfair(lr=5e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2659, 2056, 1557, 2626, 1102]\n",
      "A=0: [1472, 1314, 923, 1078, 213]\n",
      "A=1: [1187, 742, 634, 1548, 889]\n"
     ]
    }
   ],
   "source": [
    "p = model_unfair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Compute the Lagrangian loss L + lam * g\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        probs = nn.functional.softmax(preds, dim=1) # specify axis? why would it be dim 1?\n",
    "        bias = demographic_parity(probs, A)\n",
    "        loss_val += (lam * bias).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A), \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.2897000014781952 Bias: tensor([-0.0191, -0.0090, -0.0130,  0.0196,  0.0215], grad_fn=<SubBackward0>) Lambda: 0.4671611785888672\n",
      "Epoch: 39 Accuracy: 0.3239000141620636 Bias: tensor([-0.0452, -0.0360, -0.0182,  0.0100,  0.0894], grad_fn=<SubBackward0>) Lambda: 0.48258352279663086\n",
      "Epoch: 59 Accuracy: 0.383899986743927 Bias: tensor([-0.0471, -0.0348, -0.0120,  0.0163,  0.0775], grad_fn=<SubBackward0>) Lambda: 0.5010895133018494\n",
      "Epoch: 79 Accuracy: 0.47350001335144043 Bias: tensor([-0.0577, -0.0418, -0.0107,  0.0151,  0.0952], grad_fn=<SubBackward0>) Lambda: 0.5216808319091797\n",
      "Epoch: 99 Accuracy: 0.5566999912261963 Bias: tensor([-0.0679, -0.0447, -0.0041,  0.0120,  0.1048], grad_fn=<SubBackward0>) Lambda: 0.6017270088195801\n",
      "Epoch: 119 Accuracy: 0.6018000245094299 Bias: tensor([-0.0702, -0.0403,  0.0014,  0.0085,  0.1006], grad_fn=<SubBackward0>) Lambda: 0.6871916651725769\n",
      "Epoch: 139 Accuracy: 0.6258000135421753 Bias: tensor([-0.0698, -0.0346,  0.0043,  0.0065,  0.0937], grad_fn=<SubBackward0>) Lambda: 0.7634584903717041\n",
      "Epoch: 159 Accuracy: 0.6391000151634216 Bias: tensor([-0.0688, -0.0293,  0.0061,  0.0050,  0.0869], grad_fn=<SubBackward0>) Lambda: 0.8287267088890076\n",
      "Epoch: 179 Accuracy: 0.6466000080108643 Bias: tensor([-0.0672, -0.0251,  0.0072,  0.0049,  0.0802], grad_fn=<SubBackward0>) Lambda: 0.8825772404670715\n",
      "Epoch: 199 Accuracy: 0.6583999991416931 Bias: tensor([-0.0705, -0.0236,  0.0095,  0.0119,  0.0728], grad_fn=<SubBackward0>) Lambda: 0.9260808825492859\n",
      "Epoch: 219 Accuracy: 0.5153999924659729 Bias: tensor([ 0.0682,  0.0030, -0.0218, -0.0378, -0.0117], grad_fn=<SubBackward0>) Lambda: 0.9843655228614807\n",
      "Epoch: 239 Accuracy: 0.4884999990463257 Bias: tensor([ 0.0474,  0.0113, -0.0157,  0.0105, -0.0535], grad_fn=<SubBackward0>) Lambda: 0.9616104364395142\n",
      "Epoch: 259 Accuracy: 0.47589999437332153 Bias: tensor([ 0.0553,  0.0127, -0.0112,  0.0006, -0.0574], grad_fn=<SubBackward0>) Lambda: 0.9856293797492981\n",
      "Epoch: 279 Accuracy: 0.6614999771118164 Bias: tensor([-0.0690, -0.0066,  0.0067,  0.0359,  0.0329], grad_fn=<SubBackward0>) Lambda: 1.039890170097351\n",
      "Epoch: 299 Accuracy: 0.6489999890327454 Bias: tensor([-0.0495, -0.0054,  0.0190,  0.0057,  0.0302], grad_fn=<SubBackward0>) Lambda: 1.0285829305648804\n"
     ]
    }
   ],
   "source": [
    "model_fair = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9402424e-6193-4550-a458-5eae0b3557a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2084, 1611, 2614, 523, 3168]\n",
      "A=0: [1408, 1199, 1484, 278, 631]\n",
      "A=1: [676, 412, 1130, 245, 2537]\n"
     ]
    }
   ],
   "source": [
    "p = model_fair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Implement the loss function above here.\n",
    "        loss_val = ???\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25af0-dfe6-4474-8ebe-544359ff98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1ed3-96d7-4904-ae67-289010de748d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
