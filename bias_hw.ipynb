{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f56eee7-fc6d-4e47-b6da-341ccb8bfb1d",
   "metadata": {},
   "source": [
    "# Bias and Constrained Learning Homework\n",
    "\n",
    "In this homework we'll extend the constrained learning framework we used for mitigating bias in class to handle more complex situations. Specifically, we'll look at the case where the output prediction is not binary. As usual with these homeworks, there are three different levels which build on each other, each one corresponding to an increasing grade:\n",
    "\n",
    "- The basic version of this homework involves implementing code to measure fairness over multiclass classification then measuring the results when using training a regular, unfair classifier. This version is good for a C.\n",
    "- The B version of the homework involves training a classifier with some fairness constraints.\n",
    "- For an A, we'll look at slightly more complicated approach to fair training.\n",
    "\n",
    "First, we'll generate a dataset for which the sensitive attribute is binary and the output is multiclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fe355b-12d1-4afa-bd3a-87bc378c0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20455201-ce05-4a10-85ce-18ba92c47fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 5\n",
    "\n",
    "def generate_data():\n",
    "\n",
    "    dataset_size = 10000\n",
    "    dimensions = 40\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    A = np.concatenate((np.zeros(dataset_size // 2), np.ones(dataset_size // 2)))\n",
    "    rng.shuffle(A)\n",
    "    X = rng.normal(loc=A[:,np.newaxis], scale=1, size=(dataset_size, dimensions))\n",
    "    random_linear = np.array([\n",
    "        -2.28156561, 0.24582547, -2.48926942, -0.02934924, 5.21382855, -1.08613209,\n",
    "        2.51051602, 1.00773587, -2.10409448, 1.94385103, 0.76013416, -2.94430782,\n",
    "        0.3289264, -4.35145624, 1.61342623, -1.28433588, -2.07859612, -1.53812125,\n",
    "        0.51412713, -1.34310334, 4.67174476, 1.67269946, -2.07805413, 3.46667731,\n",
    "        2.61486654, 1.75418209, -0.06773796, 0.7213423, 2.43896438, 1.79306807,\n",
    "        -0.74610264, 2.84046827,  1.28779878, 1.84490263, 1.6949681, 0.05814582,\n",
    "        1.30510732, -0.92332861,  3.00192177, -1.76077192\n",
    "    ])\n",
    "    good_score = (X @ random_linear) ** 2 / 2\n",
    "    qs = np.quantile(good_score, (np.array(range(1, output_classes))) / output_classes)\n",
    "    Y = np.digitize(good_score, qs)\n",
    "\n",
    "    return X, A, Y\n",
    "\n",
    "X, A, Y = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f8bebd-2e8a-49a4-86d8-995991314ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2000, 2000, 2000, 2000, 2000]\n",
      "A=0: [1393, 1301, 1086, 826, 394]\n",
      "A=1: [607, 699, 914, 1174, 1606]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total:\", [(Y == k).sum() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((Y == k) & (A == 0)).sum() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((Y == k) & (A == 1)).sum() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1463c-e7b1-411e-b2da-c68a54791391",
   "metadata": {},
   "source": [
    "This last cell shows the total number of data points in each output category (it should be 2000 each) as well as a breakdown of each output category for the $A=0$ group and the $A=1$ group. Note that the $A=1$ group is much more likely to be assigned to the categories with higher index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d568875-652b-42eb-8318-b4d1ecf8b51c",
   "metadata": {},
   "source": [
    "## Fairness Definition (C)\n",
    "\n",
    "Let's write some code to measure the _demographic parity_ of our classifier: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $0 \\le r < K$. In the the function below,\n",
    "\n",
    "- `R` is a matrix where each row represents a probability distribution over the classes `0` to `K - 1`. That is, `R` is the output of our neural network _after_ a softmax layer.\n",
    "- `A` is a vector of sensitive attributes. Each element is either `0` or `1`.\n",
    "\n",
    "These functions should return an array of length `K` where each element of the array represents a measure of bias for _one_ of the output classes. For example, for demographic parity, the value in the output array at index `i` should be $P(R = i \\mid A = 1) - P(R = i \\mid A = 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e4b5c8a-20e5-4310-8277-c32134a01afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R is output of nn after softmax\n",
    "# A is sensitive attributes\n",
    "\n",
    "def demographic_parity(R, A):\n",
    "   # R0 = R where corresponding index in A is 0\n",
    "   # R1 same but for 1\n",
    "   R0 = R[A == 0]\n",
    "   R1 = R[A == 1]\n",
    "\n",
    "   R0_av = torch.mean(R0, axis=0)\n",
    "   R1_av = torch.mean(R1, axis=0)\n",
    "\n",
    "   output = R1_av - R0_av\n",
    "\n",
    "   return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af3e86-3594-482d-9432-2ec2f87fa4ee",
   "metadata": {},
   "source": [
    "Now we'll train a classifier on this dataset without any fairness constraints for comparison. This code is already complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09022f4a-c7d2-4023-a736-fbc950b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(40, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ec610e-dc36-4f79-bbe7-df81ee95042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unfair(lr=1e-1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A).tolist())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb30fb62-4351-4575-b784-6fa621c0d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.33660000562667847 Bias: [-0.17264802753925323, -0.15133246779441833, -0.0737253874540329, 0.022097811102867126, 0.37560802698135376]\n",
      "Epoch: 39 Accuracy: 0.35109999775886536 Bias: [-0.19326931238174438, -0.1664833426475525, -0.07638069987297058, 0.049103572964668274, 0.387029767036438]\n",
      "Epoch: 59 Accuracy: 0.38769999146461487 Bias: [-0.2048610895872116, -0.17327344417572021, -0.07213155925273895, 0.0608389675617218, 0.38942718505859375]\n",
      "Epoch: 79 Accuracy: 0.43860000371932983 Bias: [-0.20534522831439972, -0.1728249490261078, -0.06989467144012451, 0.0529661625623703, 0.3950986862182617]\n",
      "Epoch: 99 Accuracy: 0.5105000138282776 Bias: [-0.1989070326089859, -0.16608558595180511, -0.06473247706890106, 0.042642876505851746, 0.38708221912384033]\n",
      "Epoch: 119 Accuracy: 0.5820000171661377 Bias: [-0.19589506089687347, -0.16244608163833618, -0.06198650598526001, 0.04614550620317459, 0.3741821050643921]\n",
      "Epoch: 139 Accuracy: 0.6328999996185303 Bias: [-0.19359979033470154, -0.15927289426326752, -0.06181460618972778, 0.04797450453042984, 0.36671262979507446]\n",
      "Epoch: 159 Accuracy: 0.6629999876022339 Bias: [-0.1923627257347107, -0.15600767731666565, -0.06261906027793884, 0.04924387484788895, 0.36174553632736206]\n",
      "Epoch: 179 Accuracy: 0.6805999875068665 Bias: [-0.19443567097187042, -0.15442287921905518, -0.0638481080532074, 0.05657299607992172, 0.3561336398124695]\n",
      "Epoch: 199 Accuracy: 0.6783999800682068 Bias: [-0.20705334842205048, -0.16664013266563416, -0.07813268899917603, 0.11520372331142426, 0.33662253618240356]\n",
      "Epoch: 219 Accuracy: 0.7358999848365784 Bias: [-0.18190746009349823, -0.14334842562675476, -0.07105575501918793, 0.027614958584308624, 0.3686966598033905]\n",
      "Epoch: 239 Accuracy: 0.7692000269889832 Bias: [-0.16203516721725464, -0.12236759066581726, -0.05849592387676239, -8.457154035568237e-05, 0.3429831862449646]\n",
      "Epoch: 259 Accuracy: 0.7979999780654907 Bias: [-0.17330962419509888, -0.12316147983074188, -0.04235425591468811, 0.01891491562128067, 0.3199104070663452]\n",
      "Epoch: 279 Accuracy: 0.8023999929428101 Bias: [-0.18656200170516968, -0.12731745839118958, -0.045301154255867004, 0.036940768361091614, 0.32223981618881226]\n",
      "Epoch: 299 Accuracy: 0.7753999829292297 Bias: [-0.20006592571735382, -0.1348210573196411, -0.0547734797000885, 0.05597156286239624, 0.333688884973526]\n"
     ]
    }
   ],
   "source": [
    "model_unfair = train_unfair(lr=5e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c7da734-6f06-4c3d-9e43-d508749840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2765, 2082, 1301, 2706, 1146]\n",
      "A=0: [1452, 1365, 761, 1191, 231]\n",
      "A=1: [1313, 717, 540, 1515, 915]\n"
     ]
    }
   ],
   "source": [
    "p = model_unfair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83fff5-e631-47a9-b360-0f5f69b06949",
   "metadata": {},
   "source": [
    "This classifier is probably not going to be _extremely_ accurate, but you should be able to see the bias from the dataset reflected here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f3493-7763-4428-b0fb-7176a4fc1102",
   "metadata": {},
   "source": [
    "## Fair Training (B)\n",
    "\n",
    "Now we'll extend our fair training approach from the lab to the multiclass setting. Now since we have a bias measure for _each_ possible output class, we essentially have `output_classes` constraints that we need to satisfy. We can handle this within our Lagrange multiplier framework by simply adding extra multipliers for each constraint. That is, our new learning problem is\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i g_i(\\beta) \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\sum_i \\lambda_i \\left ( P_\\beta [ R = i \\mid A = 1 ] - P_\\beta [ R = i \\mid A = 0 ] \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Our `demographic_parity` function gives us a vector representing $g_i(\\beta)$, so now all we need to do is replace our single parameter $\\lambda$ from the lab with a vector then compute the dot product of $\\lambda$ with our demographic parity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5167ac-47b9-459a-8866-a22f9167b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fair(lr=1e-1, lam_lr=1, epochs=200):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.zeros(output_classes))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Compute the Lagrangian loss L + lam * g\n",
    "        preds = network(data_in)\n",
    "        loss_val = loss(preds, data_out)\n",
    "        probs = nn.functional.softmax(preds, dim=1)\n",
    "        bias = demographic_parity(probs, A)\n",
    "        loss_val += (lam * bias).sum()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            probs = nn.functional.softmax(preds, dim=1)\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Bias:\", demographic_parity(probs, A).tolist(), \"Lambda:\", lam.max().item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5523b6c-1990-4468-9d6b-fd06603cce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.2777000069618225 Bias: [-0.03208662569522858, -0.02460445463657379, -0.015192016959190369, 0.006254330277442932, 0.06562879681587219] Lambda: 0.43797749280929565\n",
      "Epoch: 39 Accuracy: 0.3305000066757202 Bias: [-0.044726788997650146, -0.03772194683551788, -0.017514914274215698, 0.015473887324333191, 0.08448979258537292] Lambda: 0.4614896774291992\n",
      "Epoch: 59 Accuracy: 0.3986999988555908 Bias: [-0.048048824071884155, -0.03726594150066376, -0.010533913969993591, 0.02355434000492096, 0.07229435443878174] Lambda: 0.4849717319011688\n",
      "Epoch: 79 Accuracy: 0.49149999022483826 Bias: [-0.06287842243909836, -0.046243295073509216, -0.009594187140464783, 0.021553561091423035, 0.09716236591339111] Lambda: 0.5110504031181335\n",
      "Epoch: 99 Accuracy: 0.5742999911308289 Bias: [-0.0725829005241394, -0.04754588007926941, -0.0022411197423934937, 0.017890378832817078, 0.10447946190834045] Lambda: 0.6065122485160828\n",
      "Epoch: 119 Accuracy: 0.6227999925613403 Bias: [-0.07398375123739243, -0.04178693890571594, 0.003428041934967041, 0.013619795441627502, 0.098722904920578] Lambda: 0.7007724046707153\n",
      "Epoch: 139 Accuracy: 0.6460999846458435 Bias: [-0.07297860085964203, -0.03528907895088196, 0.006252601742744446, 0.010891184210777283, 0.09112381935119629] Lambda: 0.7808034420013428\n",
      "Epoch: 159 Accuracy: 0.6570000052452087 Bias: [-0.07039973139762878, -0.030118480324745178, 0.007685557007789612, 0.008365228772163391, 0.08446744084358215] Lambda: 0.8477590680122375\n",
      "Epoch: 179 Accuracy: 0.661899983882904 Bias: [-0.06752099841833115, -0.02558612823486328, 0.008876502513885498, 0.005995839834213257, 0.07823476195335388] Lambda: 0.9035589098930359\n",
      "Epoch: 199 Accuracy: 0.6604999899864197 Bias: [-0.06264923512935638, -0.021859019994735718, 0.00959700345993042, 0.0026376694440841675, 0.07227358222007751] Lambda: 0.9496099352836609\n",
      "Epoch: 219 Accuracy: 0.6491000056266785 Bias: [-0.05422072112560272, -0.018459707498550415, 0.009715884923934937, -0.004657760262489319, 0.06762227416038513] Lambda: 0.9872543811798096\n",
      "Epoch: 239 Accuracy: 0.7035999894142151 Bias: [-0.06399917602539062, -0.017743900418281555, 0.011930644512176514, 0.02387946844100952, 0.04593297839164734] Lambda: 1.0170769691467285\n",
      "Epoch: 259 Accuracy: 0.5170000195503235 Bias: [0.06080418825149536, 0.004310354590415955, -0.0038019567728042603, -0.039242565631866455, -0.02207005023956299] Lambda: 1.0545434951782227\n",
      "Epoch: 279 Accuracy: 0.6263999938964844 Bias: [-0.048008039593696594, -0.00780680775642395, 0.028734371066093445, -0.032540567219257355, 0.059621065855026245] Lambda: 1.0750566720962524\n",
      "Epoch: 299 Accuracy: 0.6485999822616577 Bias: [-0.048743799328804016, -0.013101950287818909, 0.013018116354942322, -0.003504745662212372, 0.0523323118686676] Lambda: 1.0715279579162598\n"
     ]
    }
   ],
   "source": [
    "model_fair = train_fair(lr=5e-1, lam_lr=3e-1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9402424e-6193-4550-a458-5eae0b3557a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [2472, 1988, 936, 3501, 1103]\n",
      "A=0: [1187, 940, 503, 1705, 665]\n",
      "A=1: [1285, 1048, 433, 1796, 438]\n"
     ]
    }
   ],
   "source": [
    "p = model_fair(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68a6bf-043f-42db-9fdc-f23fac55129f",
   "metadata": {},
   "source": [
    "## Fair Training via KL-Divergence (A)\n",
    "\n",
    "Let's look back at our definition of demographic parity for the multiclass setting: $P(R = r \\mid A = 0) = P(R = r \\mid A = 1)$ for all possible output classes $r$. we could also express this by asserting $P(\\cdot \\mid A = 0)$ and $P(\\cdot \\mid A = 1)$ should be identical probability distributions. A natural measure of bias then would be to compute the KL-divergence between these two distributions, since KL-divergence is a measure of how \"different\" two distributions are. That is, we'll now solve the problem\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_\\lambda \\left ( L(\\beta) + \\lambda D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\right )\n",
    "$$\n",
    "\n",
    "However, this introduces a new complication. The KL-divergence is never negative and can only be zero if the two distributions are identical (we proved this in our first homework of the semester). That means there's no way for $\\lambda$ to ever decrease, and it will just go up forever. We can solve this by allowing a small deviation in our constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg\\min_\\beta &\\ L(\\beta) \\\\\n",
    "\\text{s.t.} &\\ D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) \\le \\epsilon\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can still represent this using a Lagrange multiplier:\n",
    "\n",
    "$$\n",
    "\\arg\\min_\\beta \\max_{\\lambda \\ge 0} \\left ( L(\\beta) + \\lambda \\left ( D_{\\textrm{KL}} \\left( P(\\cdot \\mid A = 0) \\ \\| \\ P(\\cdot \\mid A = 1) \\right) - \\epsilon \\right ) \\right )\n",
    "$$\n",
    "\n",
    "Your task now is to represent this optimization problem in the code below. I've taken care of clipping $\\lambda$ to zero for you since it's not something we've looked at in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f4aef62-e8af-42b3-9662-1057ee4e3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kl(lr=1e-1, lam_lr=1, epochs=300, epsilon=0.1):\n",
    "    \n",
    "    network = MLP()\n",
    "    lam = nn.Parameter(torch.tensor(0.0))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(network.parameters(), lr=lr)\n",
    "    lam_opt = optim.SGD([lam], lr=lam_lr, maximize=True)\n",
    "    data_in = torch.tensor(X).float()\n",
    "    data_out = torch.tensor(Y)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        preds = network(data_in)\n",
    "        probs = nn.functional.softmax(preds, dim = 1)\n",
    "\n",
    "        # P = prob((R = r) | (A = 0))\n",
    "        P = torch.mean(probs[A==0], axis = 0)\n",
    "\n",
    "        # Q = prob((R = r) | (A = 1))\n",
    "        Q = torch.mean(probs[A==1], axis = 0)\n",
    "\n",
    "        # Summation\n",
    "        kl_log = torch.log(P / Q)\n",
    "        kl_div = torch.sum(P * kl_log)\n",
    "\n",
    "        loss_val = loss(preds, data_out) + lam * (kl_div - epsilon) \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        lam_opt.zero_grad()\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        lam_opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lam.clamp_(min=0)\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            acc = (preds.argmax(dim=1) == data_out).float().mean()\n",
    "            print(\"Epoch:\", i, \"Accuracy:\", acc.item(), \"Divergence:\", kl_div.item(), \"Lambda:\", lam.item())\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f7eb37f-e90b-42fa-b036-be75060738e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Accuracy: 0.31310001015663147 Divergence: 0.06018543243408203 Lambda: 0.9450139999389648\n",
      "Epoch: 39 Accuracy: 0.33880001306533813 Divergence: 0.06900602579116821 Lambda: 1.2576714754104614\n",
      "Epoch: 59 Accuracy: 0.3749000132083893 Divergence: 0.06540164351463318 Lambda: 1.568528652191162\n",
      "Epoch: 79 Accuracy: 0.4171999990940094 Divergence: 0.06389471143484116 Lambda: 1.8622554540634155\n",
      "Epoch: 99 Accuracy: 0.46720001101493835 Divergence: 0.06748935580253601 Lambda: 2.195268392562866\n",
      "Epoch: 119 Accuracy: 0.5339000225067139 Divergence: 0.07535848766565323 Lambda: 2.6604878902435303\n",
      "Epoch: 139 Accuracy: 0.5928000211715698 Divergence: 0.0779055804014206 Lambda: 3.2578675746917725\n",
      "Epoch: 159 Accuracy: 0.6455000042915344 Divergence: 0.06048768013715744 Lambda: 3.7593770027160645\n",
      "Epoch: 179 Accuracy: 0.6549000144004822 Divergence: 0.06961138546466827 Lambda: 4.4724955558776855\n",
      "Epoch: 199 Accuracy: 0.6714000105857849 Divergence: 0.06775838136672974 Lambda: 4.896687030792236\n",
      "Epoch: 219 Accuracy: 0.6654000282287598 Divergence: 0.047053009271621704 Lambda: 5.404049873352051\n",
      "Epoch: 239 Accuracy: 0.6646000146865845 Divergence: 0.06803275644779205 Lambda: 5.804934501647949\n",
      "Epoch: 259 Accuracy: 0.6844000220298767 Divergence: 0.05408627912402153 Lambda: 6.150243759155273\n",
      "Epoch: 279 Accuracy: 0.6704000234603882 Divergence: 0.07575991749763489 Lambda: 6.479082107543945\n",
      "Epoch: 299 Accuracy: 0.6743999719619751 Divergence: 0.05038055032491684 Lambda: 6.811236381530762\n"
     ]
    }
   ],
   "source": [
    "model = train_kl(lr=3e-1, lam_lr=1, epsilon=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4af25af0-dfe6-4474-8ebe-544359ff98ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: [3582, 1840, 686, 1943, 1949]\n",
      "A=0: [1381, 1138, 355, 1015, 1111]\n",
      "A=1: [2201, 702, 331, 928, 838]\n"
     ]
    }
   ],
   "source": [
    "p = model(torch.tensor(X).float()).argmax(dim=1)\n",
    "print(\"Total:\", [(p == k).sum().item() for k in range(output_classes)])\n",
    "print(\"A=0:\", [((p == k) & (A == 0)).sum().item() for k in range(output_classes)])\n",
    "print(\"A=1:\", [((p == k) & (A == 1)).sum().item() for k in range(output_classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1ed3-96d7-4904-ae67-289010de748d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
